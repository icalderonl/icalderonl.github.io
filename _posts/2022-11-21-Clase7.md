---
layout: post
title: User centric y FAccT
---

Los sistemas recomendadores, como cualquier algoritmo, deben cumplir con criterios de justicia y combatir el sesgo de sus resultados. Esto es particularmente difícil debido a que tanto los programadores como los sets de datos contienen los sesgos de nuestra vida diaria, y que pueden ser replicados (o exagerados) por los programas computacionales, y además con la mayor complejidad de los sistemas desarrollados actualmente, el escrutinio de estos se vuelve muy complejo. 

Además de lo anterior, se debe considerar la poca madurez que tienen las tecnologías, principalmente las de redes neuronales. Como la masificación de estas lleva alrededor de una década, aún no se generan completamente los métodos que podrían darle transparencia y poder aplicar criterios de justicia en caso que se descubra que funciona con algún sesgo.

Para esto es que existen iniciativas como FAccT (Fairness, Accountability, and Transparency in Machine Learning) que busca entregar directrices sobre cómo desarrollar estos proyectos de forma que sean éticamente viables.
<br/>
Un ejemplo de problemas en sistemas recomendadores se da con el caso del software COMPAS, el que era utilizado por el sistema judicial de Estados Unidos. Este software entregaba recomendaciones acerca de la probabilidad de reincidencia de una persona juzgada. Se descubrió que las recomendaciones de este software estaban fuertemente sesgadas racialmente, entregando mayores probabilidades de reincidencia para personas afroamericanas.
<br/>
Otro caso conocido fue el algoritmo utilizado por YouTube hasta el año 2019. Este trabajaba usando dos redes neuronales de forma simultanea. El problema de estas recomendaciones fue descubierto por un ex-empleado de la empresa, usando un programa que generaba un **random walk** en las recomendaciones. Con esto descubrió que los resultados tendían a entregar que no solo no tenían relación directa, sino que además tenían un sesgo importante e incluso podrían llegar a ser nocivos. Al punto que existen movimientos políticos que buscan explotar las características de este algoritmo para poder radicalizar políticamente a las personas y generar fuertes divisiones políticas.

<br/>
Se busca que los algoritmos sean transparentes (que se pueda chequear el funcionamiento interno del programa) y que exista responsabilidad (que se pueda justificar los resultados entregados).
Tintarev, N., & Masthoff presentan la siguiente tabla con conceptos ligados a la transparencia de los modelos.


| Aim              | Definition                                  |
|------------------|---------------------------------------------|
| Transparency     | Explain how the system works                |
| Scrutability     | Allow users to tell the system it is wrong  |
| Trust            | Increase users confidence in the system     |
| Effectiveness    | Help users make good decisions              |
| Persuasiveness   | Convince users to try or buy                |
| Efficiency       | Help Users make decisions faster            |
| Satisfaction     | Increase the ease of usability or enjoyment |


### Referencias

1) https://medium.com/@mijordan3/artificial-intelligence-the-revolution-hasnt-happened-yet-5e1d5812e1e
<br/>
2) https://ieeexplore.ieee.org/document/4401070
<br/>
3) https://firstmonday.org/article/view/10419/9404
